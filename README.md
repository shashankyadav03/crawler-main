
# Crawler Main

## Introduction

Crawler Main is an advanced web crawling and scraping tool designed to efficiently navigate and extract data from websites. Its primary purpose is to aid in data collection for analysis, research, or archiving. With a focus on performance and ease of use, this crawler is an excellent tool for both beginners and experienced users in the field of data mining.

## Features

- **Efficient Crawling:** Optimized to handle large-scale web crawling with minimal resource usage.
- **Customizable Scraping:** Configurable to extract specific data types like text, images, and links.
- **User-Friendly Interface:** Simple command-line interface for ease of use.
- **Robust Error Handling:** Equipped to manage various web errors and inconsistencies gracefully.
- **Multi-threaded Processing:** Capable of handling multiple URLs simultaneously for faster data collection.

## Installation

1. Clone the repository:
   git clone https://github.com/shashankyadav03/crawler-main.git
2. Navigate to the project directory:
   cd crawler-main
3. Download haskell stack
   brew install haskell-stack

## Usage

To start using the crawler, navigate to the source directory and run:

stack build

After building

stack run show {any website} #eg: {https://www.eecs.qmul.ac.uk}


## Contributing

Contributions are welcome! If you have ideas for improvements or have found a bug, please open an issue or submit a pull request. Ensure you follow the coding standards and write tests for new features.

## License

This project is licensed under the QMUL License - see the LICENSE file in the repository for details.

## Contact

For support or queries, reach out to Shashank Yadav.

## Acknowledgments

Special thanks to the open-source community and all the contributors who have helped shape this project.
